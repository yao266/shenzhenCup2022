{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 待做 LDA 的文本 csv 文件，可以是本地文件，也可以是远程文件，一定要保证它是存在的！！！！\n",
    "source_csv_path = './../data/yyqxRowsData.csv'\n",
    "# 文本 csv 文件里面文本所处的列名,注意这里一定要填对，要不然会报错的！！！\n",
    "document_column_name = '内容'\n",
    "# 输出主题词的文件路径\n",
    "top_words_csv_path = 'top-topic-words.csv'\n",
    "# 输出各文档所属主题的文件路径\n",
    "predict_topic_csv_path = 'document-distribution.csv'\n",
    "# 可视化 html 文件路径\n",
    "html_path = 'yyqx-lda-visualization.html'\n",
    "#停用词路径\n",
    "stopword_path = './../data/stopwords.txt'\n",
    "# 选定的主题数\n",
    "n_topics = 20\n",
    "# 要输出的每个主题的前 n_top_words 个主题词数\n",
    "n_top_words = 20\n",
    "# 去除无意义字符的正则表达式\n",
    "pattern = u'[\\\\s\\\\d,.<>/?:;\\'\\\"[\\\\]{}()\\\\|~!\\t\"@#$%^&*\\\\-_=+，。\\n《》、？：；“”‘’｛｝【】（）…￥！—┄－]+'\n",
    "\n",
    "def top_words_data_frame(model: LatentDirichletAllocation,\n",
    "                         tf_idf_vectorizer: TfidfVectorizer,\n",
    "                         n_top_words: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出每个主题的前 n_top_words 个词\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    tf_idf_vectorizer : sklearn 的 TfidfVectorizer\n",
    "    n_top_words :前 n_top_words 个主题词\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    rows = []\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "    for topic in model.components_:\n",
    "        top_words = [feature_names[i]\n",
    "                     for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        rows.append(top_words)\n",
    "    columns = [f'topic word {i+1}' for i in range(n_top_words)]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 停用词列表\n",
    "def stopwordList(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath,'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "def predict_to_data_frame(model: LatentDirichletAllocation, X: np.ndarray) -> pd.DataFrame:\n",
    "    '''\n",
    "    求出文档主题概率分布情况\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn 的 LatentDirichletAllocation \n",
    "    X : 词向量矩阵\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    DataFrame: 包含主题词分布情况\n",
    "    '''\n",
    "    matrix = model.transform(X)\n",
    "    columns = [f'P(topic {i+1})' for i in range(len(model.components_))]\n",
    "    df = pd.DataFrame(matrix, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = (\n",
    "    pd.read_csv(\n",
    "        source_csv_path,\n",
    "        encoding='utf-8-sig')\n",
    "    .drop_duplicates()\n",
    "    .rename(columns={\n",
    "        document_column_name: 'text'\n",
    "    }).head(5000))\n",
    "# 设置停用词集合\n",
    "stop_words_set = set(stopwordList(stopword_path))\n",
    "# 去重、去缺失、分词\n",
    "df['cut'] = (\n",
    "    df['text']\n",
    "    .apply(lambda x: str(x))\n",
    "    .apply(lambda x: re.sub(pattern, ' ', x))\n",
    "    .apply(lambda x: \" \".join([word for word in jieba.lcut(x) if word not in stop_words_set]))\n",
    ")\n",
    "\n",
    "# 构造 tf-idf\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df['cut'])\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=50,\n",
    "    learning_method='online',\n",
    "    learning_offset=50,\n",
    "    random_state=0)\n",
    "\n",
    "# 使用 tf_idf 语料训练 lda 模型\n",
    "lda.fit(tf_idf)\n",
    "\n",
    "# 计算 n_top_words 个主题词\n",
    "top_words_df = top_words_data_frame(lda, tf_idf_vectorizer, n_top_words)\n",
    "\n",
    "# 保存 n_top_words 个主题词到 csv 文件中\n",
    "top_words_df.to_csv(top_words_csv_path, encoding='utf-8-sig', index=None)\n",
    "\n",
    "# 转 tf_idf 为数组，以便后面使用它来对文本主题概率分布进行计算\n",
    "X = tf_idf.toarray()\n",
    "\n",
    "# 计算完毕主题概率分布情况\n",
    "predict_df = predict_to_data_frame(lda, X)\n",
    "\n",
    "# 保存文本主题概率分布到 csv 文件中\n",
    "predict_df.to_csv(predict_topic_csv_path, encoding='utf-8-sig', index=None)\n",
    "\n",
    "# 使用 pyLDAvis 进行可视化\n",
    "data = pyLDAvis.sklearn.prepare(lda, tf_idf, tf_idf_vectorizer)\n",
    "pyLDAvis.save_html(data, html_path)\n",
    "# 清屏\n",
    "os.system('clear')\n",
    "# 浏览器打开 html 文件以查看可视化结果\n",
    "os.system(f'start {html_path}')\n",
    "\n",
    "print('本次生成了文件：',\n",
    "      top_words_csv_path,\n",
    "      predict_topic_csv_path,\n",
    "      html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ysongML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0993866fc1b884e2a7d061e361d77948da578e9256a42b4c5cc3f1a910b01c36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
